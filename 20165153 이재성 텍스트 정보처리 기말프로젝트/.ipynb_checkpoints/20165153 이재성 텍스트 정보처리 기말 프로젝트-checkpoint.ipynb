{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일을 불러오기 위한 xlrd\n",
    "# !pip install xlrd\n",
    "import xlrd\n",
    "\n",
    "workbook = xlrd.open_workbook(\"train_test.xlsx\")\n",
    "\n",
    "# train\n",
    "worksheet = workbook.sheet_by_index(0) # 엑셀 파일의 train sheet\n",
    "train = worksheet.col_values(3) \n",
    "y_train = worksheet.col_values(2)\n",
    "\n",
    "# test\n",
    "worksheet = workbook.sheet_by_index(1) # 엑셀 파일의 test sheet\n",
    "test = worksheet.col_values(3) \n",
    "y_test = worksheet.col_values(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(train)\n",
    "\n",
    "# 벡터화\n",
    "x_train = tokenizer.texts_to_sequences(train)\n",
    "x_test = tokenizer.texts_to_sequences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 단어 수\n",
    "total_word = len(tokenizer.word_index)\n",
    "\n",
    "# 단어 집합의 크기\n",
    "vocab_size = total_word+1\n",
    "\n",
    "import numpy as np\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.p', 'wb') as file: # 파일을 바이너리 쓰기 모드(wb)로 열기 \n",
    "    pickle.dump(tokenizer, file) # 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터의 최대 길이 확인\n",
    "# train + test 데이터 합치기\n",
    "all_data = train + test\n",
    "\n",
    "max_len = 0\n",
    "for i in all_data:\n",
    "    if max_len <= len(i):\n",
    "        max_len = len(i)\n",
    "        \n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen = 100)\n",
    "x_test = pad_sequences(x_test, maxlen = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49572, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((148738, 100), (49572, 80), (148738,), (49572,), 264, 39682)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape, max_len, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구축\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model__ = Sequential()\n",
    "\n",
    "model__.add(Embedding(vocab_size, 128)) # 임베딩\n",
    "model__.add(LSTM(64, activation='relu'))\n",
    "model__.add(Dropout(0.5))\n",
    "model__.add(Dense(32, activation='relu'))\n",
    "model__.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model__.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         5079296   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,130,817\n",
      "Trainable params: 5,130,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model__.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1190/1190 [==============================] - 184s 155ms/step - loss: 2057.1316 - accuracy: 0.8452 - val_loss: 0.3422 - val_accuracy: 0.8506\n",
      "Epoch 2/30\n",
      "1190/1190 [==============================] - 183s 153ms/step - loss: 135.6443 - accuracy: 0.8592 - val_loss: 0.4437 - val_accuracy: 0.8549\n",
      "Epoch 3/30\n",
      "1190/1190 [==============================] - 180s 151ms/step - loss: 13263.4209 - accuracy: 0.8726 - val_loss: 4.8256 - val_accuracy: 0.8445\n",
      "Epoch 4/30\n",
      "1190/1190 [==============================] - 179s 151ms/step - loss: 176649.1875 - accuracy: 0.8777 - val_loss: 311.7954 - val_accuracy: 0.8619\n",
      "Epoch 5/30\n",
      "1190/1190 [==============================] - 180s 151ms/step - loss: 5.3410 - accuracy: 0.8831 - val_loss: 0.3405 - val_accuracy: 0.8526\n",
      "Epoch 6/30\n",
      "1190/1190 [==============================] - 181s 153ms/step - loss: 1157.0393 - accuracy: 0.8877 - val_loss: 65.8061 - val_accuracy: 0.8620\n",
      "Epoch 7/30\n",
      "1190/1190 [==============================] - 180s 151ms/step - loss: 3546905088.0000 - accuracy: 0.8948 - val_loss: 1.5334 - val_accuracy: 0.8620\n",
      "Epoch 8/30\n",
      "1190/1190 [==============================] - 181s 152ms/step - loss: 29.2018 - accuracy: 0.8961 - val_loss: 101.6534 - val_accuracy: 0.8614\n",
      "Epoch 9/30\n",
      "1190/1190 [==============================] - 180s 151ms/step - loss: 31.9023 - accuracy: 0.8986 - val_loss: 20147.5879 - val_accuracy: 0.8547\n",
      "Epoch 10/30\n",
      "1190/1190 [==============================] - 186s 157ms/step - loss: 344.4847 - accuracy: 0.9049 - val_loss: 2251.6428 - val_accuracy: 0.8631\n",
      "Epoch 11/30\n",
      "1190/1190 [==============================] - 198s 166ms/step - loss: 0.5278 - accuracy: 0.9067 - val_loss: 0.3547 - val_accuracy: 0.8569\n",
      "Epoch 12/30\n",
      "1190/1190 [==============================] - 201s 169ms/step - loss: 9.9855 - accuracy: 0.8925 - val_loss: 1.7808 - val_accuracy: 0.7267\n",
      "Epoch 13/30\n",
      "1190/1190 [==============================] - 198s 166ms/step - loss: 3.0056 - accuracy: 0.8980 - val_loss: 0.3595 - val_accuracy: 0.8383\n",
      "Epoch 14/30\n",
      "1190/1190 [==============================] - 197s 166ms/step - loss: 231.9483 - accuracy: 0.9086 - val_loss: 0.3524 - val_accuracy: 0.8585\n",
      "Epoch 15/30\n",
      "1190/1190 [==============================] - 197s 165ms/step - loss: 43.6255 - accuracy: 0.9169 - val_loss: 0.3611 - val_accuracy: 0.8589\n",
      "Epoch 16/30\n",
      "1190/1190 [==============================] - 198s 166ms/step - loss: 0.2931 - accuracy: 0.9209 - val_loss: 160.3807 - val_accuracy: 0.8613\n",
      "Epoch 17/30\n",
      "1190/1190 [==============================] - 197s 166ms/step - loss: 0.6703 - accuracy: 0.9256 - val_loss: 0.3715 - val_accuracy: 0.8589\n",
      "Epoch 18/30\n",
      "1190/1190 [==============================] - 197s 166ms/step - loss: 0.3402 - accuracy: 0.9268 - val_loss: 0.4409 - val_accuracy: 0.8600\n",
      "Epoch 19/30\n",
      "1190/1190 [==============================] - 198s 166ms/step - loss: 538.0541 - accuracy: 0.9288 - val_loss: 4313.8818 - val_accuracy: 0.8569\n",
      "Epoch 20/30\n",
      "1190/1190 [==============================] - 197s 165ms/step - loss: 1022.1395 - accuracy: 0.9324 - val_loss: 249.9667 - val_accuracy: 0.8549\n",
      "Epoch 21/30\n",
      "1190/1190 [==============================] - 197s 165ms/step - loss: 7.5101 - accuracy: 0.9339 - val_loss: 0.4403 - val_accuracy: 0.8578\n",
      "Epoch 22/30\n",
      "1190/1190 [==============================] - 196s 164ms/step - loss: 1.1449 - accuracy: 0.9358 - val_loss: 0.3699 - val_accuracy: 0.8564\n",
      "Epoch 23/30\n",
      "1190/1190 [==============================] - 197s 166ms/step - loss: 382209.5000 - accuracy: 0.9225 - val_loss: 2317413.2500 - val_accuracy: 0.8520\n",
      "Epoch 24/30\n",
      "1190/1190 [==============================] - 197s 165ms/step - loss: 1959134.5000 - accuracy: 0.9167 - val_loss: 8124.0459 - val_accuracy: 0.8530\n",
      "Epoch 25/30\n",
      "1190/1190 [==============================] - 198s 166ms/step - loss: 1539.3760 - accuracy: 0.9373 - val_loss: 926189.8750 - val_accuracy: 0.8533\n",
      "Epoch 26/30\n",
      "1190/1190 [==============================] - 197s 166ms/step - loss: 123808.9922 - accuracy: 0.9379 - val_loss: 41.1250 - val_accuracy: 0.8554\n",
      "Epoch 27/30\n",
      "1190/1190 [==============================] - 198s 166ms/step - loss: 128.2631 - accuracy: 0.9377 - val_loss: 61377.7383 - val_accuracy: 0.8531\n",
      "Epoch 28/30\n",
      "1190/1190 [==============================] - 205s 172ms/step - loss: 4010112.0000 - accuracy: 0.9415 - val_loss: 21718.1660 - val_accuracy: 0.8565\n",
      "Epoch 29/30\n",
      "1190/1190 [==============================] - 200s 168ms/step - loss: 0.4158 - accuracy: 0.9420 - val_loss: 25176.7871 - val_accuracy: 0.8568\n",
      "Epoch 30/30\n",
      "1190/1190 [==============================] - 199s 167ms/step - loss: 13.0561 - accuracy: 0.9451 - val_loss: 0.4336 - val_accuracy: 0.8510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x207c567f2b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "# loss 값이 갑자기 커지는 부분이 있어 에포크를 30, 100개 마다 확인하게 설정, 과적합 발생 가능성.\n",
    "model__.fit(x_train, y_train, epochs = 30, batch_size = 100, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model__.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "\n",
    "workbook = xlrd.open_workbook(\"test.xlsx\")\n",
    "\n",
    "# test\n",
    "worksheet = workbook.sheet_by_index(0) # test sheet에 접근합니다.\n",
    "test = worksheet.col_values(0) # 기본 data\n",
    "test_morpheme = worksheet.col_values(1) # kkma를 이용하여 형태소 분석한 결과\n",
    "test_syllable = worksheet.col_values(2) # 음절별로 나눈 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenzier load\n",
    "\n",
    "with open('tokenizer.p', 'rb') as file: # 파일을 바이너리 읽기 모드(rb)로 열기\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(map(str, test))\n",
    "\n",
    "# 벡터화\n",
    "test_data = tokenizer.texts_to_sequences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in test_data:\n",
    "    if max_len <= len(i):\n",
    "        max_len = len(i)\n",
    "        \n",
    "# 패딩\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_data = pad_sequences(test_data, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-ed6c22034792>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "y_pred = model__.predict_classes(test_data).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "\n",
    "write = Workbook()\n",
    "write_ = write.create_sheet(\"y값\")\n",
    "\n",
    "write_ = write.active\n",
    "\n",
    "# 값 추가 / 행 단위\n",
    "for i in y_pred:\n",
    "    write_.append([i])\n",
    "    \n",
    "# 저장\n",
    "write.save(\"20165153 이재성 예측한 값.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
