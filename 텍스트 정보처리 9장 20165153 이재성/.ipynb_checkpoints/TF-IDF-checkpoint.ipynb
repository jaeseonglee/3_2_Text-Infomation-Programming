{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF_IDF 생성 후 심층 신경망을 이용한 이메일 분류\n",
    "# 20개의 사전 분류된 범주 중 하나로 이메일을 분류 하기 위해 DNN(심층 신경망)을 사용(281p)\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train') # 처음 다운로드에 많은 시간이 소요\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = newsgroups_train.data\n",
    "x_test = newsgroups_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20개 카테고리 전체 목록:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "\n",
      "샘플 이메일:\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "샘플 타겟 카테고리:\n",
      "7\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "print (\"20개 카테고리 전체 목록:\")\n",
    "print (newsgroups_train.target_names)\n",
    "print (\"\\n\")\n",
    "print (\"샘플 이메일:\")\n",
    "print (x_train[0])\n",
    "print (\"샘플 타겟 카테고리:\")\n",
    "print (y_train[0])\n",
    "print (newsgroups_train.target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리에 사용\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text): # (284p ~ 289p))\n",
    "    # 표준 문장부호가 있다면 빈칸으로 바꾸고 아니면 공백으로 바꾸지 않는다. \n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in \n",
    "              nltk.word_tokenize(sent)] # 공백에 따라 단어로 토큰화하고 추가 단계를 적용하기 위한 리스트로 함께 묶는다.\n",
    "    \n",
    "    tokens = [word.lower() for word in tokens] # 모든 문자를 소문자로 변환해 말뭉치에서 중복을 제거\n",
    "    \n",
    "    stopwds = stopwords.words('english') \n",
    "    tokens = [token for token in tokens if token not in stopwds] # 불용어 제거\n",
    "     \n",
    "    tokens = [word for word in tokens if len(word)>=3] # 길이가 3이상인 단어만 남기고 다른 단어 제거\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens] # 접미사가 나오는 단어에 PorterStemmer 함수를 통해 스테밍\n",
    "        \n",
    "    tagged_corpus = pos_tag(tokens) # 품사 태깅\n",
    "    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS'] # NN: 명사 일반 단수, NNP: 명사 보통 복수  \n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ'] # VB: 동사 기본, VBD 동사 현재, VBN: 동사 과거\n",
    "    lemmatizer = WordNetLemmatizer()                 # VBP: 동사 현재 3인칭 단수가 아닌 것, VBZ: 동사 현재 3인칭 단수\n",
    "\n",
    "    def prat_lemmatize(token,tag): # tag값이 leematize 함수의 값이 일치하지 않는 경우\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    \n",
    "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
    "\n",
    "    return pre_proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 데이터 전처리 , 실행이 오래걸림\n",
    "x_train_preprocessed  = []\n",
    "for i in x_train:\n",
    "    x_train_preprocessed .append(preprocessing(i))\n",
    "\n",
    "x_test_preprocessed = []\n",
    "for i in x_test:\n",
    "    x_test_preprocessed.append(preprocessing(i))\n",
    "\n",
    "# TFIDF 벡터라이저(vectorizer) 구축\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),  stop_words='english', \n",
    "                             max_features= 10000,strip_accents='unicode',  norm='l2')\n",
    "\n",
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391, 564)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_preprocessed[0]), len(x_train_preprocessed[1]) # 교재에는 없지만 조교님이 데이터 확인을 위해 쓰신 코드같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lerxst wam umd edu thing subject car nntp post host rac3 wam umd edu organ univers maryland colleg park line wonder anyon could enlighten car saw day door sport car look late 60 earli 70 call bricklin door realli small addit front bumper separ rest bodi know anyon tellm model name engin spec year product car make histori whatev info funki look car plea mail thank bring neighborhood lerxst'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_preprocessed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 딥러닝 모듈 (286p~ )\n",
    "#!pip install tensorflow  // keras가 tensorflow를 필요로 해서 설치\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta,Adam,RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337) \n",
    "nb_classes = 20 # 클래스 20\n",
    "batch_size = 64 # 일괄 처리 사이즈 64\n",
    "nb_epochs = 20  # 학습할 에포크 수는 20으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0] # 역시 교재에는 없는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1000)              10001000  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                25050     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 10,527,570\n",
      "Trainable params: 10,527,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 케라스에서의 딥 러이어 (심층) 모델 구축\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000,input_shape= (10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "177/177 [==============================] - 17s 97ms/step - loss: 1.9635\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 19s 106ms/step - loss: 0.6165\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 21s 120ms/step - loss: 0.30670s - loss:\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 18s 103ms/step - loss: 0.18431s -\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 20s 111ms/step - loss: 0.1234\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 20s 112ms/step - loss: 0.0850\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 19s 108ms/step - loss: 0.0678\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 16s 92ms/step - loss: 0.0618\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 17s 95ms/step - loss: 0.0467\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 17s 95ms/step - loss: 0.0434\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 20s 115ms/step - loss: 0.0399\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 19s 107ms/step - loss: 0.03940s - los\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 20s 112ms/step - loss: 0.0366\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 20s 115ms/step - loss: 0.0389\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 21s 117ms/step - loss: 0.0295\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 20s 112ms/step - loss: 0.0289\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 17s 96ms/step - loss: 0.0266\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 15s 88ms/step - loss: 0.0252\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 16s 91ms/step - loss: 0.0277\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 19s 105ms/step - loss: 0.0241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2239dba7fd0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1) # 모델 학습 # 각 에포크마다 학습 하기에 오래 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras 모듈 때문인지 데이터 때문인지를 모르겠는데 위의 loss 값이 교재와 다른 것을 확인할 수 있다.\n",
    "\n",
    "y_train_predclass = model.predict_classes(x_train_2, batch_size=batch_size)\n",
    "y_test_predclass = model.predict_classes(x_test_2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Deep Neural Network  - Train accuracy:\n",
      "\n",
      "Deep Neural Network  - Test accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 0.809)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래서 그런 것인지 위의 모델 학습 이후의 코드의 결과가 교재와는 다른 점들이 보인다.\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "print (\"\\n\\nDeep Neural Network  - Train accuracy:\"),(round(accuracy_score(y_train,y_train_predclass),3))\n",
    "print (\"\\nDeep Neural Network  - Test accuracy:\"),(round(accuracy_score(y_test,y_test_predclass),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deep Neural Network  - Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       480\n",
      "           1       1.00      1.00      1.00       584\n",
      "           2       1.00      1.00      1.00       591\n",
      "           3       1.00      0.99      1.00       590\n",
      "           4       1.00      1.00      1.00       578\n",
      "           5       1.00      1.00      1.00       593\n",
      "           6       1.00      0.99      1.00       585\n",
      "           7       1.00      1.00      1.00       594\n",
      "           8       1.00      1.00      1.00       598\n",
      "           9       1.00      1.00      1.00       597\n",
      "          10       1.00      1.00      1.00       600\n",
      "          11       1.00      1.00      1.00       595\n",
      "          12       0.99      1.00      1.00       591\n",
      "          13       1.00      1.00      1.00       594\n",
      "          14       1.00      1.00      1.00       593\n",
      "          15       1.00      1.00      1.00       599\n",
      "          16       1.00      1.00      1.00       546\n",
      "          17       1.00      1.00      1.00       564\n",
      "          18       1.00      1.00      1.00       465\n",
      "          19       1.00      1.00      1.00       377\n",
      "\n",
      "    accuracy                           1.00     11314\n",
      "   macro avg       1.00      1.00      1.00     11314\n",
      "weighted avg       1.00      1.00      1.00     11314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nDeep Neural Network  - Train Classification Report\")\n",
    "print (classification_report(y_train,y_train_predclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deep Neural Network  - Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78       319\n",
      "           1       0.66      0.73      0.69       389\n",
      "           2       0.72      0.68      0.70       394\n",
      "           3       0.70      0.67      0.68       392\n",
      "           4       0.79      0.75      0.77       385\n",
      "           5       0.85      0.76      0.80       395\n",
      "           6       0.83      0.78      0.80       390\n",
      "           7       0.88      0.85      0.86       396\n",
      "           8       0.85      0.93      0.89       398\n",
      "           9       0.90      0.91      0.90       397\n",
      "          10       0.94      0.96      0.95       399\n",
      "          11       0.95      0.88      0.91       396\n",
      "          12       0.59      0.79      0.68       393\n",
      "          13       0.85      0.82      0.83       396\n",
      "          14       0.91      0.90      0.91       394\n",
      "          15       0.88      0.86      0.87       398\n",
      "          16       0.78      0.86      0.82       364\n",
      "          17       0.97      0.85      0.91       376\n",
      "          18       0.74      0.66      0.70       310\n",
      "          19       0.64      0.68      0.66       251\n",
      "\n",
      "    accuracy                           0.81      7532\n",
      "   macro avg       0.81      0.80      0.81      7532\n",
      "weighted avg       0.82      0.81      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nDeep Neural Network  - Test Classification Report\")\n",
    "print (classification_report(y_test,y_test_predclass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
